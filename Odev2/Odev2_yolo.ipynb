{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./assets/images/\"\n",
    "\n",
    "image_files = glob.glob(os.path.join(file_path, \"*.jpg\")) + glob.glob(os.path.join(file_path, \"*.jpeg\"))  \n",
    "print(len(image_files))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run YOLO for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/cars.jpg: 384x640 5 cars, 1 bus, 3 trucks, 793.3ms\n",
      "Speed: 2.2ms preprocess, 793.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/berlin.jpg: 384x640 1 person, 945.0ms\n",
      "Speed: 2.5ms preprocess, 945.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/sailingrace.jpg: 480x640 6 persons, 6 boats, 1070.3ms\n",
      "Speed: 3.3ms preprocess, 1070.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/balikmezati.jpg: 384x640 8 persons, 1 banana, 1 chair, 1 dining table, 855.2ms\n",
      "Speed: 2.5ms preprocess, 855.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/trafalgar_square.jpg: 448x640 4 persons, 1 bus, 1 backpack, 899.0ms\n",
      "Speed: 2.3ms preprocess, 899.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/restaurant.jpg: 384x640 9 persons, 1 cup, 1 chair, 2 dining tables, 828.6ms\n",
      "Speed: 3.4ms preprocess, 828.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/dining_table.jpg: 640x640 5 wine glasss, 3 forks, 3 knifes, 1 spoon, 1 bowl, 4 chairs, 1 potted plant, 1 dining table, 1557.0ms\n",
      "Speed: 7.2ms preprocess, 1557.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/cafe_1.jpg: 384x640 10 persons, 10 chairs, 1 potted plant, 817.5ms\n",
      "Speed: 3.8ms preprocess, 817.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/salon.jpeg: 416x640 1 bowl, 1 couch, 2 dining tables, 2 vases, 872.9ms\n",
      "Speed: 5.5ms preprocess, 872.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "image 1/1 /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/assets/images/cats&dogs.jpeg: 384x640 2 cats, 1 dog, 842.6ms\n",
      "Speed: 2.8ms preprocess, 842.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = YOLO(\"yolo11x.pt\")\n",
    "i = 1\n",
    "for image_file in image_files:\n",
    "    results = model(image_file,conf=0.45,stream=False,save=True,device='cpu')\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "        masks = result.masks  # Masks object for segmentation masks outputs\n",
    "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "        probs = result.probs  # Probs object for classification outputs\n",
    "        obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "        result.show()  # display to screen\n",
    "        result.save(filename=f\"./assets/results/result_{i}.jpg\") \n",
    "    i += 1    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run YOLO for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 4 cars, 1126.4ms\n",
      "Speed: 5.9ms preprocess, 1126.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 753.0ms\n",
      "Speed: 2.6ms preprocess, 753.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 739.7ms\n",
      "Speed: 2.3ms preprocess, 739.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 757.2ms\n",
      "Speed: 1.7ms preprocess, 757.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 774.9ms\n",
      "Speed: 1.7ms preprocess, 774.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 827.9ms\n",
      "Speed: 1.5ms preprocess, 827.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 833.0ms\n",
      "Speed: 1.9ms preprocess, 833.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 783.9ms\n",
      "Speed: 1.5ms preprocess, 783.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 732.1ms\n",
      "Speed: 1.5ms preprocess, 732.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 810.2ms\n",
      "Speed: 1.7ms preprocess, 810.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 798.7ms\n",
      "Speed: 1.8ms preprocess, 798.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 853.3ms\n",
      "Speed: 1.8ms preprocess, 853.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 837.5ms\n",
      "Speed: 1.9ms preprocess, 837.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 778.4ms\n",
      "Speed: 1.8ms preprocess, 778.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 781.8ms\n",
      "Speed: 1.8ms preprocess, 781.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 739.1ms\n",
      "Speed: 1.7ms preprocess, 739.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 755.5ms\n",
      "Speed: 1.7ms preprocess, 755.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 823.4ms\n",
      "Speed: 1.6ms preprocess, 823.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 834.0ms\n",
      "Speed: 1.6ms preprocess, 834.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 795.3ms\n",
      "Speed: 2.0ms preprocess, 795.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 814.8ms\n",
      "Speed: 1.7ms preprocess, 814.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 793.9ms\n",
      "Speed: 1.8ms preprocess, 793.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 791.4ms\n",
      "Speed: 1.9ms preprocess, 791.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 1210.3ms\n",
      "Speed: 13.5ms preprocess, 1210.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1072.9ms\n",
      "Speed: 1.8ms preprocess, 1072.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 995.5ms\n",
      "Speed: 1.9ms preprocess, 995.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 821.9ms\n",
      "Speed: 1.8ms preprocess, 821.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 844.5ms\n",
      "Speed: 1.9ms preprocess, 844.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 852.1ms\n",
      "Speed: 2.8ms preprocess, 852.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 822.9ms\n",
      "Speed: 2.0ms preprocess, 822.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 813.9ms\n",
      "Speed: 1.6ms preprocess, 813.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 826.5ms\n",
      "Speed: 1.9ms preprocess, 826.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 1001.8ms\n",
      "Speed: 1.9ms preprocess, 1001.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 891.0ms\n",
      "Speed: 1.7ms preprocess, 891.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 9 cars, 866.6ms\n",
      "Speed: 2.1ms preprocess, 866.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 966.8ms\n",
      "Speed: 2.6ms preprocess, 966.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 914.0ms\n",
      "Speed: 1.4ms preprocess, 914.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 809.1ms\n",
      "Speed: 2.0ms preprocess, 809.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 9 cars, 1016.8ms\n",
      "Speed: 1.9ms preprocess, 1016.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 863.2ms\n",
      "Speed: 2.6ms preprocess, 863.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 810.5ms\n",
      "Speed: 1.9ms preprocess, 810.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 854.3ms\n",
      "Speed: 1.8ms preprocess, 854.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 858.3ms\n",
      "Speed: 2.2ms preprocess, 858.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 827.9ms\n",
      "Speed: 1.8ms preprocess, 827.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 870.8ms\n",
      "Speed: 1.8ms preprocess, 870.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 815.1ms\n",
      "Speed: 1.8ms preprocess, 815.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 831.4ms\n",
      "Speed: 2.0ms preprocess, 831.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 848.0ms\n",
      "Speed: 1.7ms preprocess, 848.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 882.6ms\n",
      "Speed: 1.8ms preprocess, 882.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 863.5ms\n",
      "Speed: 2.2ms preprocess, 863.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 909.9ms\n",
      "Speed: 2.2ms preprocess, 909.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 865.5ms\n",
      "Speed: 2.0ms preprocess, 865.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 818.6ms\n",
      "Speed: 1.8ms preprocess, 818.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 821.3ms\n",
      "Speed: 1.9ms preprocess, 821.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 790.2ms\n",
      "Speed: 2.0ms preprocess, 790.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 843.9ms\n",
      "Speed: 1.8ms preprocess, 843.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 864.0ms\n",
      "Speed: 1.8ms preprocess, 864.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 825.4ms\n",
      "Speed: 2.2ms preprocess, 825.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 1089.7ms\n",
      "Speed: 2.0ms preprocess, 1089.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 1076.2ms\n",
      "Speed: 2.4ms preprocess, 1076.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 1057.7ms\n",
      "Speed: 2.2ms preprocess, 1057.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 828.0ms\n",
      "Speed: 2.1ms preprocess, 828.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 911.7ms\n",
      "Speed: 2.0ms preprocess, 911.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 8 cars, 860.1ms\n",
      "Speed: 2.0ms preprocess, 860.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 847.1ms\n",
      "Speed: 2.1ms preprocess, 847.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 1047.8ms\n",
      "Speed: 42.8ms preprocess, 1047.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 885.9ms\n",
      "Speed: 2.1ms preprocess, 885.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 844.8ms\n",
      "Speed: 1.8ms preprocess, 844.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 929.4ms\n",
      "Speed: 2.6ms preprocess, 929.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 7 cars, 1008.5ms\n",
      "Speed: 1.9ms preprocess, 1008.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 841.2ms\n",
      "Speed: 1.9ms preprocess, 841.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 1 motorcycle, 835.2ms\n",
      "Speed: 1.9ms preprocess, 835.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 826.9ms\n",
      "Speed: 1.8ms preprocess, 826.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 829.4ms\n",
      "Speed: 1.9ms preprocess, 829.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 954.8ms\n",
      "Speed: 1.9ms preprocess, 954.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 873.3ms\n",
      "Speed: 1.9ms preprocess, 873.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 2 motorcycles, 1113.6ms\n",
      "Speed: 2.4ms preprocess, 1113.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 910.8ms\n",
      "Speed: 2.3ms preprocess, 910.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 947.5ms\n",
      "Speed: 2.1ms preprocess, 947.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 3 cars, 2 motorcycles, 910.7ms\n",
      "Speed: 2.2ms preprocess, 910.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 1 motorcycle, 843.1ms\n",
      "Speed: 1.9ms preprocess, 843.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 818.1ms\n",
      "Speed: 1.8ms preprocess, 818.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 3 cars, 1 motorcycle, 1041.4ms\n",
      "Speed: 1.8ms preprocess, 1041.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 898.4ms\n",
      "Speed: 2.1ms preprocess, 898.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 982.2ms\n",
      "Speed: 1.9ms preprocess, 982.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 883.0ms\n",
      "Speed: 2.3ms preprocess, 883.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 999.9ms\n",
      "Speed: 2.0ms preprocess, 999.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1236.5ms\n",
      "Speed: 1.9ms preprocess, 1236.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 905.1ms\n",
      "Speed: 1.8ms preprocess, 905.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 853.1ms\n",
      "Speed: 1.8ms preprocess, 853.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 1 motorcycle, 856.3ms\n",
      "Speed: 1.8ms preprocess, 856.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 817.5ms\n",
      "Speed: 4.0ms preprocess, 817.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 780.1ms\n",
      "Speed: 1.7ms preprocess, 780.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 789.3ms\n",
      "Speed: 1.8ms preprocess, 789.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 819.4ms\n",
      "Speed: 1.8ms preprocess, 819.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 1 motorcycle, 810.8ms\n",
      "Speed: 1.6ms preprocess, 810.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 1 motorcycle, 811.7ms\n",
      "Speed: 1.8ms preprocess, 811.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 3 cars, 1 motorcycle, 794.2ms\n",
      "Speed: 1.8ms preprocess, 794.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 2 motorcycles, 805.9ms\n",
      "Speed: 1.8ms preprocess, 805.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 2 motorcycles, 770.1ms\n",
      "Speed: 1.9ms preprocess, 770.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 4 cars, 2 motorcycles, 836.3ms\n",
      "Speed: 1.7ms preprocess, 836.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 1 motorcycle, 857.5ms\n",
      "Speed: 2.8ms preprocess, 857.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 6 cars, 1 motorcycle, 1010.7ms\n",
      "Speed: 2.0ms preprocess, 1010.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n",
      "0: 640x384 5 cars, 1 motorcycle, 925.6ms\n",
      "Speed: 1.8ms preprocess, 925.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m success, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# YOLO modeliyle tespit işlemini yap\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Tespit edilmiş kareyi çerçeve içinde göster\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:461\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 461\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:262\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3(torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(x)), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:346\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/activation.py:393\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:2074\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 2074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_path = \"./assets/test_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO inference on the frame\n",
    "        results = model(frame,save=True, device = 'cpu')\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 handbag, 1 suitcase, 1 bed, 800.7ms\n",
      "video 1/1 (frame 2/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 suitcase, 1 bottle, 1 bed, 1017.0ms\n",
      "video 1/1 (frame 3/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 suitcase, 1 bottle, 1 bed, 850.4ms\n",
      "video 1/1 (frame 4/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bottle, 1 bed, 838.4ms\n",
      "video 1/1 (frame 5/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bottle, 1 bed, 839.2ms\n",
      "video 1/1 (frame 6/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bottle, 1 bed, 827.1ms\n",
      "video 1/1 (frame 7/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 1012.2ms\n",
      "video 1/1 (frame 8/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 1002.3ms\n",
      "video 1/1 (frame 9/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 883.4ms\n",
      "video 1/1 (frame 10/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 854.2ms\n",
      "video 1/1 (frame 11/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 2 bottles, 1 bed, 879.1ms\n",
      "video 1/1 (frame 12/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 2 bottles, 1 bed, 858.8ms\n",
      "video 1/1 (frame 13/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 886.1ms\n",
      "video 1/1 (frame 14/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 suitcase, 1 bed, 873.0ms\n",
      "video 1/1 (frame 15/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 857.0ms\n",
      "video 1/1 (frame 16/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 886.0ms\n",
      "video 1/1 (frame 17/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 suitcase, 1 bed, 908.3ms\n",
      "video 1/1 (frame 18/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 1380.8ms\n",
      "video 1/1 (frame 19/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1 bed, 1360.5ms\n",
      "video 1/1 (frame 20/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 1431.1ms\n",
      "video 1/1 (frame 21/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1 bed, 1454.1ms\n",
      "video 1/1 (frame 22/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bed, 1903.1ms\n",
      "video 1/1 (frame 23/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bottle, 1 chair, 1 bed, 1063.6ms\n",
      "video 1/1 (frame 24/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1 bed, 1131.1ms\n",
      "video 1/1 (frame 25/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1052.6ms\n",
      "video 1/1 (frame 26/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 bottle, 1 chair, 1 bed, 1258.0ms\n",
      "video 1/1 (frame 27/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1 bed, 1366.1ms\n",
      "video 1/1 (frame 28/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1 bed, 1610.9ms\n",
      "video 1/1 (frame 29/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 suitcase, 1 chair, 1 bed, 1352.3ms\n",
      "video 1/1 (frame 30/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1077.8ms\n",
      "video 1/1 (frame 31/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1157.6ms\n",
      "video 1/1 (frame 32/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1086.5ms\n",
      "video 1/1 (frame 33/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 backpack, 1 chair, 1535.5ms\n",
      "video 1/1 (frame 34/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1373.3ms\n",
      "video 1/1 (frame 35/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 suitcase, 1 chair, 914.3ms\n",
      "video 1/1 (frame 36/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1216.7ms\n",
      "video 1/1 (frame 37/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 cell phone, 992.6ms\n",
      "video 1/1 (frame 38/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 901.1ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The application /System/Applications/Preview.app cannot be opened for an unexpected reason, error=Error Domain=NSOSStatusErrorDomain Code=-600 \"procNotFound: no eligible process with specified descriptor\" UserInfo={_LSLine=388, _LSFunction=_LSAnnotateAndSendAppleEventWithOptions}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (frame 39/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 738.7ms\n",
      "video 1/1 (frame 40/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 cell phone, 892.5ms\n",
      "video 1/1 (frame 41/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 cell phone, 823.1ms\n",
      "video 1/1 (frame 42/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 cell phone, 862.0ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The application /System/Applications/Preview.app cannot be opened for an unexpected reason, error=Error Domain=NSOSStatusErrorDomain Code=-600 \"procNotFound: no eligible process with specified descriptor\" UserInfo={_LSLine=388, _LSFunction=_LSAnnotateAndSendAppleEventWithOptions}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (frame 43/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 736.6ms\n",
      "video 1/1 (frame 44/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 857.0ms\n",
      "video 1/1 (frame 45/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 cell phone, 810.5ms\n",
      "video 1/1 (frame 46/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 903.3ms\n",
      "video 1/1 (frame 47/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1008.9ms\n",
      "video 1/1 (frame 48/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 mouse, 978.9ms\n",
      "video 1/1 (frame 49/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1089.4ms\n",
      "video 1/1 (frame 50/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 tv, 1065.0ms\n",
      "video 1/1 (frame 51/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1013.3ms\n",
      "video 1/1 (frame 52/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 laptop, 819.1ms\n",
      "video 1/1 (frame 53/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 2 beds, 819.5ms\n",
      "video 1/1 (frame 54/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 laptop, 846.3ms\n",
      "video 1/1 (frame 55/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 cell phone, 867.2ms\n",
      "video 1/1 (frame 56/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 cell phone, 868.2ms\n",
      "video 1/1 (frame 57/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 cell phone, 855.8ms\n",
      "video 1/1 (frame 58/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 cell phone, 853.4ms\n",
      "video 1/1 (frame 59/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 2 remotes, 866.1ms\n",
      "video 1/1 (frame 60/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 remote, 1 cell phone, 849.1ms\n",
      "video 1/1 (frame 61/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 remote, 825.9ms\n",
      "video 1/1 (frame 62/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 remote, 857.6ms\n",
      "video 1/1 (frame 63/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 remote, 844.3ms\n",
      "video 1/1 (frame 64/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 2 remotes, 883.1ms\n",
      "video 1/1 (frame 65/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 remote, 943.0ms\n",
      "video 1/1 (frame 66/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 remote, 942.1ms\n",
      "video 1/1 (frame 67/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 cell phone, 896.3ms\n",
      "video 1/1 (frame 68/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 1 bed, 1 cell phone, 886.8ms\n",
      "video 1/1 (frame 69/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 1 bed, 1 remote, 1 cell phone, 931.4ms\n",
      "video 1/1 (frame 70/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 remote, 1885.0ms\n",
      "video 1/1 (frame 71/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 bed, 1 remote, 1451.8ms\n",
      "video 1/1 (frame 72/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 remote, 935.0ms\n",
      "video 1/1 (frame 73/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 tv, 943.1ms\n",
      "video 1/1 (frame 74/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 tv, 2 remotes, 947.2ms\n",
      "video 1/1 (frame 75/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1136.8ms\n",
      "video 1/1 (frame 76/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 2 remotes, 1093.8ms\n",
      "video 1/1 (frame 77/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 remote, 900.6ms\n",
      "video 1/1 (frame 78/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 861.3ms\n",
      "video 1/1 (frame 79/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 908.5ms\n",
      "video 1/1 (frame 80/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 891.8ms\n",
      "video 1/1 (frame 81/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 885.2ms\n",
      "video 1/1 (frame 82/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 919.4ms\n",
      "video 1/1 (frame 83/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 882.7ms\n",
      "video 1/1 (frame 84/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 938.1ms\n",
      "video 1/1 (frame 85/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 book, 924.6ms\n",
      "video 1/1 (frame 86/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bed, 1 book, 987.8ms\n",
      "video 1/1 (frame 87/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 bed, 1 book, 913.7ms\n",
      "video 1/1 (frame 88/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 bed, 1 book, 970.3ms\n",
      "video 1/1 (frame 89/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 bed, 1 cell phone, 839.1ms\n",
      "video 1/1 (frame 90/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 couch, 1 bed, 1 cell phone, 908.4ms\n",
      "video 1/1 (frame 91/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 couch, 1 bed, 1 cell phone, 890.6ms\n",
      "video 1/1 (frame 92/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 couch, 1 bed, 1 cell phone, 880.6ms\n",
      "video 1/1 (frame 93/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 couch, 1 bed, 1 cell phone, 924.5ms\n",
      "video 1/1 (frame 94/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 couch, 1 bed, 969.4ms\n",
      "video 1/1 (frame 95/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 couch, 1 bed, 1 cell phone, 953.5ms\n",
      "video 1/1 (frame 96/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 couch, 1 bed, 1 cell phone, 1010.8ms\n",
      "video 1/1 (frame 97/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 bed, 1 cell phone, 968.2ms\n",
      "video 1/1 (frame 98/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1 cell phone, 934.7ms\n",
      "video 1/1 (frame 99/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 938.3ms\n",
      "video 1/1 (frame 100/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 1 dining table, 959.4ms\n",
      "video 1/1 (frame 101/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1023.6ms\n",
      "video 1/1 (frame 102/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 3 bottles, 1 wine glass, 1 cup, 1 couch, 1 dining table, 1022.6ms\n",
      "video 1/1 (frame 103/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 2 couchs, 1 dining table, 1148.6ms\n",
      "video 1/1 (frame 104/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 2 couchs, 1 dining table, 1140.9ms\n",
      "video 1/1 (frame 105/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 3 wine glasss, 1 cup, 2 couchs, 1 dining table, 1 tv, 1291.8ms\n",
      "video 1/1 (frame 106/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1328.6ms\n",
      "video 1/1 (frame 107/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1511.6ms\n",
      "video 1/1 (frame 108/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1 tv, 1098.8ms\n",
      "video 1/1 (frame 109/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1 tv, 1029.3ms\n",
      "video 1/1 (frame 110/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 1 dining table, 1 tv, 1124.7ms\n",
      "video 1/1 (frame 111/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1022.2ms\n",
      "video 1/1 (frame 112/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 1 dining table, 940.3ms\n",
      "video 1/1 (frame 113/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 970.9ms\n",
      "video 1/1 (frame 114/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 2 couchs, 1 dining table, 960.0ms\n",
      "video 1/1 (frame 115/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 knife, 2 couchs, 1 dining table, 984.5ms\n",
      "video 1/1 (frame 116/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1163.0ms\n",
      "video 1/1 (frame 117/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 3 wine glasss, 2 cups, 1 couch, 1 dining table, 957.8ms\n",
      "video 1/1 (frame 118/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 3 wine glasss, 2 cups, 1 couch, 1 dining table, 958.2ms\n",
      "video 1/1 (frame 119/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 940.5ms\n",
      "video 1/1 (frame 120/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 knife, 1 couch, 1 dining table, 969.9ms\n",
      "video 1/1 (frame 121/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1012.3ms\n",
      "video 1/1 (frame 122/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 982.7ms\n",
      "video 1/1 (frame 123/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 1 dining table, 1030.0ms\n",
      "video 1/1 (frame 124/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1 remote, 995.8ms\n",
      "video 1/1 (frame 125/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1023.0ms\n",
      "video 1/1 (frame 126/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 wine glasss, 1 cup, 1 couch, 1 dining table, 1 remote, 1091.5ms\n",
      "video 1/1 (frame 127/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 1 cup, 1 couch, 1 dining table, 1 remote, 1112.5ms\n",
      "video 1/1 (frame 128/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 wine glass, 2 cups, 1 couch, 1 dining table, 1 remote, 1105.3ms\n",
      "video 1/1 (frame 129/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 wine glasss, 2 cups, 1 couch, 1 dining table, 1 remote, 1 cell phone, 1064.4ms\n",
      "video 1/1 (frame 130/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 wine glasss, 3 cups, 1 couch, 1 dining table, 1 remote, 1070.5ms\n",
      "video 1/1 (frame 131/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 wine glasss, 3 cups, 1 couch, 1 dining table, 1 remote, 1100.3ms\n",
      "video 1/1 (frame 132/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 3 wine glasss, 1 knife, 1 couch, 1 remote, 1062.8ms\n",
      "video 1/1 (frame 133/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 wine glasss, 4 cups, 1 couch, 1 dining table, 1 remote, 1027.1ms\n",
      "video 1/1 (frame 134/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 3 cups, 1 couch, 1 dining table, 1 remote, 1050.8ms\n",
      "video 1/1 (frame 135/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 3 cups, 1 couch, 1 dining table, 1 remote, 1 scissors, 1059.1ms\n",
      "video 1/1 (frame 136/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 wine glass, 4 cups, 1 couch, 1 remote, 1091.3ms\n",
      "video 1/1 (frame 137/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 4 cups, 1 couch, 1 remote, 1 scissors, 1052.3ms\n",
      "video 1/1 (frame 138/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1 scissors, 1084.1ms\n",
      "video 1/1 (frame 139/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1 scissors, 1104.3ms\n",
      "video 1/1 (frame 140/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1 scissors, 1078.8ms\n",
      "video 1/1 (frame 141/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1 scissors, 1077.0ms\n",
      "video 1/1 (frame 142/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1175.6ms\n",
      "video 1/1 (frame 143/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 spoon, 1 couch, 1 remote, 1117.1ms\n",
      "video 1/1 (frame 144/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1028.7ms\n",
      "video 1/1 (frame 145/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1111.5ms\n",
      "video 1/1 (frame 146/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 remote, 1175.5ms\n",
      "video 1/1 (frame 147/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 couch, 1 remote, 1198.2ms\n",
      "video 1/1 (frame 148/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 couch, 1 sink, 1094.8ms\n",
      "video 1/1 (frame 149/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 couch, 1 sink, 1098.8ms\n",
      "video 1/1 (frame 150/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 couch, 1 sink, 1162.2ms\n",
      "video 1/1 (frame 151/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 suitcase, 1 sink, 1012.0ms\n",
      "video 1/1 (frame 152/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1038.6ms\n",
      "video 1/1 (frame 153/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1131.8ms\n",
      "video 1/1 (frame 154/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1005.0ms\n",
      "video 1/1 (frame 155/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1050.6ms\n",
      "video 1/1 (frame 156/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 laptop, 1013.2ms\n",
      "video 1/1 (frame 157/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1062.8ms\n",
      "video 1/1 (frame 158/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 laptop, 1012.4ms\n",
      "video 1/1 (frame 159/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1029.8ms\n",
      "video 1/1 (frame 160/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1102.4ms\n",
      "video 1/1 (frame 161/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1172.9ms\n",
      "video 1/1 (frame 162/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1051.1ms\n",
      "video 1/1 (frame 163/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1069.1ms\n",
      "video 1/1 (frame 164/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1092.8ms\n",
      "video 1/1 (frame 165/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1180.4ms\n",
      "video 1/1 (frame 166/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1123.1ms\n",
      "video 1/1 (frame 167/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1106.4ms\n",
      "video 1/1 (frame 168/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1171.4ms\n",
      "video 1/1 (frame 169/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1201.7ms\n",
      "video 1/1 (frame 170/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1147.9ms\n",
      "video 1/1 (frame 171/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1052.3ms\n",
      "video 1/1 (frame 172/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1141.9ms\n",
      "video 1/1 (frame 173/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1100.3ms\n",
      "video 1/1 (frame 174/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1063.3ms\n",
      "video 1/1 (frame 175/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1073.1ms\n",
      "video 1/1 (frame 176/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1644.5ms\n",
      "video 1/1 (frame 177/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1320.6ms\n",
      "video 1/1 (frame 178/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1231.0ms\n",
      "video 1/1 (frame 179/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1063.8ms\n",
      "video 1/1 (frame 180/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1027.0ms\n",
      "video 1/1 (frame 181/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 830.4ms\n",
      "video 1/1 (frame 182/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 761.4ms\n",
      "video 1/1 (frame 183/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1118.1ms\n",
      "video 1/1 (frame 184/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 1150.0ms\n",
      "video 1/1 (frame 185/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 911.7ms\n",
      "video 1/1 (frame 186/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 805.9ms\n",
      "video 1/1 (frame 187/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 801.5ms\n",
      "video 1/1 (frame 188/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 813.9ms\n",
      "video 1/1 (frame 189/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 824.1ms\n",
      "video 1/1 (frame 190/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 837.4ms\n",
      "video 1/1 (frame 191/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 810.6ms\n",
      "video 1/1 (frame 192/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 804.0ms\n",
      "video 1/1 (frame 193/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 831.9ms\n",
      "video 1/1 (frame 194/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 815.3ms\n",
      "video 1/1 (frame 195/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 821.7ms\n",
      "video 1/1 (frame 196/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 801.4ms\n",
      "video 1/1 (frame 197/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 852.5ms\n",
      "video 1/1 (frame 198/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 808.5ms\n",
      "video 1/1 (frame 199/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 809.4ms\n",
      "video 1/1 (frame 200/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 802.7ms\n",
      "video 1/1 (frame 201/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 toilet, 828.7ms\n",
      "video 1/1 (frame 202/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 812.6ms\n",
      "video 1/1 (frame 203/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 819.0ms\n",
      "video 1/1 (frame 204/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 943.0ms\n",
      "video 1/1 (frame 205/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 899.4ms\n",
      "video 1/1 (frame 206/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 865.0ms\n",
      "video 1/1 (frame 207/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 832.8ms\n",
      "video 1/1 (frame 208/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 819.8ms\n",
      "video 1/1 (frame 209/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 844.6ms\n",
      "video 1/1 (frame 210/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 875.9ms\n",
      "video 1/1 (frame 211/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 826.7ms\n",
      "video 1/1 (frame 212/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 827.9ms\n",
      "video 1/1 (frame 213/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 834.5ms\n",
      "video 1/1 (frame 214/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 chairs, 842.3ms\n",
      "video 1/1 (frame 215/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 843.6ms\n",
      "video 1/1 (frame 216/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 830.8ms\n",
      "video 1/1 (frame 217/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 841.9ms\n",
      "video 1/1 (frame 218/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 oven, 1 refrigerator, 856.0ms\n",
      "video 1/1 (frame 219/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 906.9ms\n",
      "video 1/1 (frame 220/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 790.8ms\n",
      "video 1/1 (frame 221/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 1103.7ms\n",
      "video 1/1 (frame 222/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 sink, 934.2ms\n",
      "video 1/1 (frame 223/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 sink, 803.9ms\n",
      "video 1/1 (frame 224/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 refrigerator, 781.4ms\n",
      "video 1/1 (frame 225/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 refrigerator, 1019.0ms\n",
      "video 1/1 (frame 226/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 refrigerator, 800.1ms\n",
      "video 1/1 (frame 227/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 1 sink, 791.7ms\n",
      "video 1/1 (frame 228/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 799.1ms\n",
      "video 1/1 (frame 229/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 1 bowl, 785.1ms\n",
      "video 1/1 (frame 230/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 2 bowls, 812.0ms\n",
      "video 1/1 (frame 231/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 2 bowls, 796.0ms\n",
      "video 1/1 (frame 232/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bottles, 3 bowls, 1 oven, 789.2ms\n",
      "video 1/1 (frame 233/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 1 bowl, 787.9ms\n",
      "video 1/1 (frame 234/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 2 bowls, 823.4ms\n",
      "video 1/1 (frame 235/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 3 bowls, 807.1ms\n",
      "video 1/1 (frame 236/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bottle, 3 bowls, 786.0ms\n",
      "video 1/1 (frame 237/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 bowls, 787.7ms\n",
      "video 1/1 (frame 238/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 787.7ms\n",
      "video 1/1 (frame 239/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 cup, 1 dining table, 826.5ms\n",
      "video 1/1 (frame 240/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 809.9ms\n",
      "video 1/1 (frame 241/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 799.0ms\n",
      "video 1/1 (frame 242/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 832.2ms\n",
      "video 1/1 (frame 243/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 865.9ms\n",
      "video 1/1 (frame 244/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 dining table, 850.3ms\n",
      "video 1/1 (frame 245/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 cup, 1 chair, 1 dining table, 819.8ms\n",
      "video 1/1 (frame 246/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 cup, 1 chair, 1 dining table, 822.2ms\n",
      "video 1/1 (frame 247/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 (no detections), 798.7ms\n",
      "video 1/1 (frame 248/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 1 dining table, 822.9ms\n",
      "video 1/1 (frame 249/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 bowl, 2 chairs, 1 dining table, 814.7ms\n",
      "video 1/1 (frame 250/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 cups, 1 bowl, 2 chairs, 1 dining table, 854.9ms\n",
      "video 1/1 (frame 251/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 cups, 1 bowl, 2 chairs, 1 dining table, 820.9ms\n",
      "video 1/1 (frame 252/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 2 cups, 1 chair, 810.6ms\n",
      "video 1/1 (frame 253/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 cup, 2 chairs, 2 dining tables, 865.3ms\n",
      "video 1/1 (frame 254/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 cup, 1 chair, 1 dining table, 834.1ms\n",
      "video 1/1 (frame 255/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 826.3ms\n",
      "video 1/1 (frame 256/256) /Users/alperalanyali/Desktop/robot_dreams_r_d/Odev2/test_video.mp4: 640x384 1 chair, 834.1ms\n",
      "Speed: 4.0ms preprocess, 980.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = model('test_video.mp4',save=True,show=True,stream=True)\n",
    "\n",
    "for result in results:\n",
    "    result.show()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
